index <-which(x != 1)
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- apply(lyr_cleaned,1, get.terms)
documents
D <- length(documents)  # number of documents (2,000)
W <- length(vocab_m)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
K <- 10
G <- 5000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(357)
doc.length
documents
documents[1]
documents[[1]]
get.terms <- function(x) {
index <-which(x != 0)
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- apply(lyr_cleaned,1, get.terms)
documents
documents <- apply(lyr_cleaned,1, get.terms)
##
D <- length(documents)  # number of documents (2,000)
W <- length(vocab_m)  # number of terms in the vocab (14,568)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (546,827)
doc.length
N <- sum(doc.length)  # total number of tokens in the data (546,827)
term.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]
K <- 10
G <- 5000
alpha <- 0.02
eta <- 0.02
library(lda)
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab_m,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
Lyrics <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab_m,
term.frequency = term.frequency)
library(LDAvis)
# create the JSON object to feed the visualization:
json <- createJSON(phi = Lyrics$phi,
theta = Lyrics$theta,
doc.length = Lyrics$doc.length,
vocab = Lyrics$vocab_m,
term.frequency = Lyrics$term.frequency)
Lyrics <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab_m,
term.frequency = term.frequency)
length(Lyrics$phi)
length(Lyrics$theta)
json <- createJSON(phi = Lyrics$phi,
theta = Lyrics$theta,
doc.length = Lyrics$doc.length,
vocab = Lyrics$vocab_m,
term.frequency = Lyrics$term.frequency)
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
dim(phi)
phi
Lyrics <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab_m,
term.frequency = term.frequency)
Lyrics$phi
json <- createJSON(phi = Lyrics$phi,
theta = Lyrics$theta,
doc.length = Lyrics$doc.length,
vocab = Lyrics$vocab_m,
term.frequency = Lyrics$term.frequency)
json <- createJSON(phi = Lyrics$phi,
theta = Lyrics$theta,
doc.length = Lyrics$doc.length,
vocab = Lyrics$vocab,
term.frequency = Lyrics$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
serVis(json, out.dir = 'vis', open.browser = FALSE)
getwd()
setwd("~/Deskto")
setwd("~/Desktop")
serVis(json, out.dir = 'vis', open.browser = FALSE)
install.packages("servr")
serVis(json, out.dir = 'vis', open.browser = FALSE)
serVis(json, out.dir = 'vis', open.browser = FALSE)
ï¼ŸLyrics
?Lyrics
?fit
? lda.collapsed.gibbs.sampler
fit$document_sums
fit$document_expects
ass <- fit$document_expects
ass <- fit$document_expects
max(ass[,2])
which(ass[,2]==max(ass[,2]))
which(ass[,3]==max(ass[,3]))
assignments <- apply(ass,2,function(x) which(x==max(x)))
assignments
document[1:10,]
tracks <- rownames(documents)
tracks
rownames(documents)
tracks <- row.names(documents)
tracks
documents[1:10]
tracks <- names(documents)
tracks
m<- cbind(tracks,assignments)
write.csv(m,file="~/Desktop/cluster.csv")
m
unique(m[,2])
simulateBm <- function(n){
interval <- 1/n
bm <- matrix(nrow = 10,ncol = n)
x <- matrix(nrow = 10,ncol = n)
bm[,1] <- rnorm(10,0,interval)
for (j in 1:10){
las <- 0
x[j,] <- seq(interval,1,interval)
for (i in 1:n){
bm[j,i] <- las + rnorm(1,0,interval)
las <- bm[j,i]
}
}
matplot(t(x),t(bm),type = "l", lty = 1:10, lwd = 1, pch = NULL,col = 1:10,main = paste("n = ",n))
}
simulateBm(10)
rexp(1)
rexp(1)
simulateHm <- function( mu, k, sigma){
n <- 1000
interval <- 1/n
m <- matrix(nrow = 10,ncol = n)
x <- matrix(nrow = 10,ncol = n)
bm[,1] <- rnorm(10,0,interval)
for (j in 1:10){
las <- 0
x[j,] <- seq(interval,1,interval)
for (i in 1:n){
dBt <- rnorm(1,0,interval)
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
while ((las+dX) < 0){
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
}
m[j,i]<-las + dX
las <- m[j,i]
}
}
matplot(t(x),t(m),type = "l", lty = 1:10, lwd = 1, pch = NULL,col = 1:10,main = paste("a = ",a," b = ",b,"k = ",k,"v = ",v,"sigma = ", sigma))
}
simulateHm(1,1,1)
simulateHm <- function( mu, k, sigma){
n <- 1000
interval <- 1/n
m <- matrix(nrow = 10,ncol = n)
x <- matrix(nrow = 10,ncol = n)
m[,1] <- rnorm(10,0,interval)
for (j in 1:10){
las <- 0
x[j,] <- seq(interval,1,interval)
for (i in 1:n){
dBt <- rnorm(1,0,interval)
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
while ((las+dX) < 0){
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
}
m[j,i]<-las + dX
las <- m[j,i]
}
}
matplot(t(x),t(m),type = "l", lty = 1:10, lwd = 1, pch = NULL,col = 1:10,main = paste("a = ",a," b = ",b,"k = ",k,"v = ",v,"sigma = ", sigma))
}
simulateHm(1,1,1)
simulate <- function( mu, k, sigma){
n <- 1000
interval <- 1/n
m <- matrix(nrow = 10,ncol = n)
x <- matrix(nrow = 10,ncol = n)
m[,1] <- rnorm(10,0,interval)
for (j in 1:10){
las <- 0
x[j,] <- seq(interval,1,interval)
for (i in 1:n){
dBt <- rnorm(1,0,interval)
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
while ((las+dX) < 0){
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
}
m[j,i]<-las + dX
las <- m[j,i]
}
}
matplot(t(x),t(m),type = "l", lty = 1:10, lwd = 1, pch = NULL,col = 1:10,main = paste("k = ",k,"mu = ",mu,"sigma = ", sigma))
}
simulate(1,1,1)
simulate(0.5,0.5,0.5)
simulate(0.05,0.05,0.5)
simulate(0.005,0.05,0.5)
simulate(0.005,0.005,0.5)
simulate(1,0.005,0.5)
simulate(10,0.005,0.5)
simulate(10,0.001,0.001)
simulate <- function( mu, k, sigma){
n <- 1000
interval <- 1/n
m <- matrix(nrow = 10,ncol = n)
x <- matrix(nrow = 10,ncol = n)
m[,1] <- rnorm(10,0,interval)
for (j in 1:10){
las <- 0
x[j,] <- seq(interval,1,interval)
for (i in 1:n){
dBt <- rnorm(1,0,interval)
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
while ((las+dX) < 0){
print(las+dX)
dX <- k*(mu-las)*interval + sigma*dBt*sqrt(las)+rexp(1)
}
m[j,i]<-las + dX
las <- m[j,i]
}
}
matplot(t(x),t(m),type = "l", lty = 1:10, lwd = 1, pch = NULL,col = 1:10,main = paste("k = ",k,"mu = ",mu,"sigma = ", sigma))
}
simulate(10,0.001,0.001)
simulate(1,0.001,0.001)
simulate(1,0.001,0.001)
simulate(1,0.0001,0.001)
simulate(1,10,0.001)
simulate(1,100,0.001)
simulate(1,100,1)
simulate(1,5,1)
simulate(2,5,10)
simulate(2,7,10)
simulate(0.1,1,1)
simulate(5,1,1)
simulate(0.1,7,10)
simulate(2,1,10)
simulate(2,1,1)
simulate(2,1,10)
data <- read.csv("~/Downloads/train_kagglebank.csv")
names(data)
vars <- apply(data,2,var)
vars
sum_vars <- sum(vars)
vars% <- vars/sum_vars
vars_percentage <- vars/sum_vars
vars_percentage
vars/sum_vars
sum_vars
pca<- prcomp(data)
pca
names(pca)
pca$sdev
sds <- pca$sdev
sds_percentage <- sds/sum(sds)
sds_percentage
sds_percentage <- cumsum(sds)/sum(sds)
sds_percentage
selected <- pca[,1:19]
pca$rotation
pca$rotation[,1:19]
selected <- pca$rotation[,1:19]
head(selected)
index <- which(sds_percentage>0.99)
index
index <- which(sds_percentage>0.99)[0]
selected <- pca$rotation[,1:index]
index <- which(sds_percentage>0.99)[0]
selected <- pca$rotation[,1:index]
index <- which(sds_percentage>0.99)[1]
selected <- pca$rotation[,1:index]
selected
names(pca)
pca$x
sds_percentage
selected<-pca$x[,1:index]
selected
names(pca
)
?prcomp
?princomp
?prcomp
train_data <- read.csv("~/Downloads/train_kagglebank.csv")
pca$sdev
test_data <- read.csv("~/Downloads/test.csv")
dim(test_data)
dim(train_data)
names(test_data)
ass <- fit$document_expects
ass
assignments <- apply(ass,2,function(x) which(x==max(x)))
assignments
hist(assignments)
?fit
?lda.collapsed.gibbs.sampler
fit$topics
ass <- fit$document_sums
assignments <- apply(ass,2,function(x) which(x==max(x)))
hist(assignments)
assignments
hist(unlist(assignments))
fit$topics
fit$assignments
fit$topic_sums
fit$log.likelihoods
fit$topic_sums
fit$document_sums
fit$document_sums
fit$document_sums
ass <- fit$document_sums
which(ass[,1]!=0)
a <- which(ass[,1]!=0)
a
ass <- fit$document_sums
assignments <- apply(ass,2,function(x) which(x!=max(x)))
assignments
length(unlist(assignments))
length(assignments)
length(unlist(assignments))
hist(unlist(assignments))
assignments <- apply(ass,2,function(x) which(x!=0)))
tracks <- names(documents)
m<- cbind(tracks,assignments)
write.csv(m,file="~/Desktop/cluster.csv")
hist(unlist(assignments))
assignments <- apply(ass,2,function(x) which(x!=0))
tracks <- names(documents)
m<- cbind(tracks,assignments)
write.csv(m,file="~/Desktop/cluster.csv")
hist(unlist(assignments))
length(unlist(assignments))
aassignments
assignments
assignments <- apply(ass,2,function(x) which(x!=0))
tracks <- names(documents)
m<- cbind(tracks,assignments)
hist(unlist(assignments))
hist(unlist(assignments))
fit$document_sums
assignments
length(assignments)
assignments
assignments[1]
assignments[1][1]
assignments[[1]]
assignments[[1]][1]
result <- c()
for(i in 1:length(assignments)){
n = length(assignments[[i]])
for(j in 1:n){
m<- cbind(tracks[i],assignments[[i]][j])
result <- rbind(result,m)
}
}
result
dim(result)
length(unlist(assignments))
assignments[[1]
]
assignments[[2]]
assignments[[3]]
head(result)
head(track)
head(tracks)
write.csv(result,file="~/Desktop/cluster_topics.csv")
result
sum(fit$log.likelihoods)
sum(exp(fit$log.likelihoods))
exp(fit$log.likelihoods)
fit$log.likelihoods
fit$log.likelihoods
sum(fit$log.likelihoods)
log_l <- c()
for (K in 1:12){
fit2 <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab_m,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
print(sum(fit2$log.likelihoods))
log_l[K] <- sum(fit2$log.likelihoods)
}
log_l
log_l2 <- c()
for (K in 13:18){
fit3 <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab_m,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
print(sum(fit3$log.likelihoods))
log_l2[K] <- sum(fit3$log.likelihoods)
}
log_l + log_12
vector(log_l, log_12)
rbind(log_l, log_12)
cbind(log_l, log_12)
log_l
log_12
log_l2
log_l2[1:10] <- log_l[1:10]
log_l2
log_l2[1:10] <- log_l[1:12]
log_l2[1:12] <- log_l[1:12]
log_l2
plot(log_l2)
plot(log_l2,type = b)
plot(log_l2,type = 'b')
plot(log_l2,type = 'b',label=1:9)
K <- 9
G <- 5000
alpha <- 0.02
eta <- 0.02
# Fit the model:
library(lda)
set.seed(357)
#t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab_m,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
dim(result)
is.na(result[,1])
sum(is.na(result[,1]))
sum(is.na(result[,2]))
hist(unlist(assignments))
fit
K= 10
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab_m,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
Lyrics
names(Lyrics)
Lyrics$vocab
Lyrics
names(Lyrics)
phi
theta
phi
phi[c(1,6,10)]
phi[c(1,6,10),]
words<-phi[c(1,6,10),]
words
rownames(phi)<-c("1","2",'3','4','5','6','7','8','9','10')
words<-phi[c(1,6,10),]
words
words[1]
words[1,]
sort(words[1,])
head(sort(words[1,]))
sort(words[1,])[1:20]
setwd("~/Desktop/finalproject-group-2/data")
data <- read.csv("topicSummary.csv")
data
wordcloud(data, max.words = 100, random.order = FALSE)
library(tm)
library(SnowballC)
library(wordcloud)
install.packages("wordcloud")
wordcloud(data, max.words = 100, random.order = FALSE)
library(wordcloud)
wordcloud(data, max.words = 100, random.order = FALSE)
wordcloud(data.frame(data), max.words = 100, random.order = FALSE)
data <- as.data.frame(data)
library(wordcloud)
wordcloud(data, max.words = 100, random.order = FALSE)
Corpus(data)
Corpus(VectorSource(data))
data <- Corpus(VectorSource(data))
wordcloud(data, max.words = 100, random.order = FALSE)
library(wordcloud)
wordcloud(data, random.order = FALSE)
data <- Corpus(VectorSource(data))
data <- read.csv("topicSummary.csv")
data <- as.data.frame(data)
data2 <- Corpus(VectoreSource(data))
library(wordcloud)
data2 <- Corpus(VectoreSource(data))
data
data2 <- Corpus(VectoreSource(data[,-3]))
data2 <- Corpus(VectorSource(data[,-3]))
data2
wordcloud(data2, random.order = FALSE)
tm_map(data2)
name(data2)
names(data2)
data2[1]
data2[,1]
data2[1]
data2[2]
data2
wordcloud(data2, random.order = FALSE)
wordcloud(word=data[,1],frequency(data[,2]), random.order = FALSE)
wordcloud(words =  data[,1],frequency =data[,2], random.order = FALSE)
library(RColorBrewer)
wordcloud(words =  data[,1],frequency =data[,2],colors=brewer.pal(8,'Dark2'), random.order = FALSE)
wordcloud(words =  data[,1],frequency =data[,2],colors=brewer.pal(8,'Dark2'))
?wordcloud
wordcloud(words =  data[,1],freq =data[,2],colors=brewer.pal(8,'Dark2'))
